# Neural Network Cycle Count Prediction Model 

## Abstract
Machine learning is a relatively new technology that is being used in almost every industry today. However, one of the fields that fail to take advantage of its power is the processor design realm. In order to begin to bridge the gap between machine learning and processor design, we developed a machine learning model that predicts the performance of a CPU pipeline design using a simulator. To do so, we went through the process of collecting and generating data and developing a long short term memory (LSTM) model that predicts the total number of cycles taken for a given basic block. This model provided some accuracy using TensorFlows accuracy metric for models, but can be further improved upon using a more complex and optimized LSTM model, or modifying the way the data is being preprocessed. 

## Background
Research into using machine learning for computer architecture and its components is a new topic of study, but can inevitably be utilized to understand, research, and optimize implementations of microprocessors to its full potential. Machine learning has been changing every industry for the last decade, and will eventually also transform the way processors are designed and tested. For this reason, we decided to work on a project that coupled machine learning with CPU pipelines by creating a deep neural network to estimate the total number of execution cycles. As a result, we are attempting to advance the exploration and connection between computer architecture and machine learning. 

## Problem
CPU cycles and execution time are both very important metrics in determining the performance of a processor. Currently, benchmark computing tests that run a set of programs through a pipeline are used to determine a CPU’s performance because simply analyzing a computer's specifications is a difficult and inaccurate way to judge its true capabilities. Furthermore, while comparing CPUs by their specifications allows you to analyze the tradeoffs between each type of individual metric, there is still no concrete answer for which CPU will run faster and perform better for the system it is being used for without actually testing it. However, using a machine learning model to accurately predict CPU performance would make it easier and less costly to implement, test, and evaluate the performance of a microprocessor. If such a model existed, chip designers, companies, and researchers could determine whether their new innovative architectures and technologies were performing well without actually having to test the designs themselves; rather, the machine learning model would give a reasonable estimate of the CPU simulator’s capabilities and limitations.

## Architectural Solution
Our solution to this problem of expensive testing and implementation was to create a machine learning model that predicts the total numbers of cycles taken for a given CPU. The model would be trained on a large set of programs in assembly that were each mapped to the total cycle count associated with the CPU simulator it was run through. The ISA would be limited to RISC-V and the dataset would only contain basic blocks to eliminate looping, branching, and assembly code bugs that could occur in large scale programs. As a result of narrowing down the ISA specifications and getting rid of jump instructions, we would yield more accurate results for a specific ISA because the amount of instructions the model needs to recognize would be far less and the total cycle count would be directly related to the length of the basic block. Once trained on a specific CPU, the model would then be able to accurately output the total cycle count given a RISC-V program. 

The model itself consists of several layers. The first LSTM layer takes in a vector of mapped tokens with each index being an instruction from the given basic block. The LSTM output was fed into a Rectifier (ReLU) linear layer which removed all negative values from the first LSTM layer output of the model using a piecewise function. The second layer of the model is an LSTM layer that allows us to generate predictions based on the previous instructions that were executed in the basic block. This layer was then fed into two subsequent linear layers made up of a Softsign and a Scaled Exponential Linear Unit (SeLU) to flatten and normalize the prediction values from the LSTM. The output of this model was an array of cycle counts for each individual instruction. After summing all elements from this output array, the model was able to provide an estimate for the total cycle count per basic block. Figure 1 below demonstrates how the model processes one specific instruction of a basic block.

![alt text](https://github.com/abbywysopal/cs254_project/blob/main/ArchitecturalSolution.png)

Once perfected, the model would be flexible enough to be trained on different CPU simulators so that researchers and companies could test their new architectural designs and get accurate estimates for their performance. If the results were unsatisfactory, they could revise the CPU simulator and test it again through the machine learning model until they received their desired performance specifications. At this point, the processor could be physically implemented and tested. As a result, the cost of developing the processor would decrease because the initial iterations of the chip wouldn’t have to be physically implemented.


## Method of evaluation
The primary method of evaluation for our model was the built-in accuracy metric given by the TensorFlow fit function. The accuracy metric uses the total number of validation tests and the number of correct predictions to compute the frequency of accurate predictions by dividing the two values for a given validation dataset. This metric allowed us to determine the performance of the model after being fitted and tested on the validation data. As the model went through each epoch, the loss function decreased meaning the actual values became closer to the predicted values and the accuracy became better. 

Furthermore, we also created our own evaluation method to keep track of the prediction accuracy in comparison to the true cycle count of the basic block. To do this, we took the sum of the output array of cycle counts per instruction and compared it to the true value of the total number of execution cycles for a given basic block. 


## Implementation progress

Our project can be broken down into two phases. The first phase was collecting and processing the data necessary to train and test the model. This is one of the most important aspects when developing a machine learning model because it determines the quality and quantity of the training, validation, and testing data. Once we had our dataset, the next step was to create our machine learning model. This required an extensive amount of researching and experimenting with many different types of layers, parameters, and data preprocessing. 

To collect our data, we attempted a variety of different solutions. Because this was one of the most important components to developing accurate predictions for neural networks, a large part of our project was dedicated to formatting and synthesizing our datasets. We began with determining the type of data we wanted to gather and how we wanted to format it. Initially, our dataset was made up of images of instructions being executed in a pipeline with the thought of training an image classification model. After further research, we realized image classification proved to be inefficient because assembly code is able to be interpreted as a form of a natural language. The next datasets we tried to find online were those that contained relationships between assembly code and CPU performance. We looked into a variety of metrics to evaluate the performance of a CPU including time per cycle, total execution time, and total execution cycles, but were ultimately unable to find a large and relevant enough dataset that we could use to properly train our model. 

After unsuccessfully finding datasets online, we decided to create our own assembly code generator. We found an open sourced project online that generated an RISC-V assembly code program called RISC-V Test Generator. This allowed us to generate tens of thousands of assembly code programs, each with an instruction count between two and thirty two. To optimize this assembly code into basic blocks for our model, we modified the code base to eliminate loops, branching instructions, and general assembly code bugs we found as mentioned previously. Furthermore, the types of instructions we used in the basic blocks were limited to 14 fundamental instructions: WRS, WR, LDI, ADDI, SUBI, MULI, DIVI, STORE, LOAD, XOR, ADD, SUB, MUL, and DIV. This smaller subset of instructions allowed our machine learning model to create more accurate predictions. This narrowed down the number of opcodes for the model to process through the use of more focused data points. To preprocess our data, we tokenized, vectorized, and padded our data. The initial format of our data mapped each basic block to their corresponding cycle count given by the CPU simulator. Next, an XML markup was created through the tokenization of each basic block with block, instr, opcode, srcs, operand, and dsts tokens. This enabled the model to parse through the different aspects of the basic block and label them accordingly. Finally, each XML markup was indexed into a lookup table, vectorized, and padded to allow for vectors of varying lengths to be treated equally. After the dataset was generated and preprocessed, each basic block from the dataset was run through an open-sourced 5-stage pipelined CPU simulator. The simulator recorded the number of cycles per instruction and the total number of execution cycles per basic block. 

Throughout the creation of our machine learning model, we attempted several types of models to find the best fit for our project. The initial model we tried was a linear regression model. With an input of basic blocks composed of hexadecimal instruction, the model attempted to output the number of cycles per basic block. Unfortunately, the model was unsuccessful in accurately predicting the correct number of execution cycles. As a result, we made modifications to the linear regression model. The new input further broke down the hexadecimal instructions into basic blocks of binary instructions and outputted a binary array that represented the number of execution cycles. This model did in fact produce a higher accuracy than the previous linear model; however, the output binary array itself was not an accurate representation of the true cycle count. The next type of model we used was a survival model. This model took an input of each instruction and its duration, then outputted the life expectancy of the basic block. Because the model did not give us a performance metric, we decided to use an LSTM model. This became our final model since it was a combination of long short term memory (LSTM) layers and linear layers. By attempting several models before the LSTM model, we were able to gain a better understanding of machine learning components and their functionality, which ultimately helped us develop a successful final model. An illustration and explanation of our machine learning model is given in the Architectural Solution section of this report. 

## Results

The machine learning model’s average prediction accuracy is 10%. Figure 2 displays the machine learning model’s prediction accuracy when tested on datasets of differing basic block instruction count size. The size of the basic block increases from two instructions (leftmost column) to thirty-two instructions (rightmost column) on the x-axis, and its corresponding prediction accuracy percentage is on the y-axis. 

![alt text](https://github.com/abbywysopal/cs254_project/blob/main/Results.png)

## Installation
$ git clone https://github.com/abbywysopal/cs254_project

## Compile and Run
$ python run.py

Generate dataset to feed into ML model

$ python LSTM_model.py

Create and train ML model
